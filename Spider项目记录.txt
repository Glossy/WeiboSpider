-------------------------------2017.6.8--------------------------------------------------
项目基本的架构还没想好，用java还是python还没确定。万事开头难，搜集一天资料。最好找到有最近更新过的github项目follow。
打算使用模拟浏览器的策略来进行微博的爬取，因为虽然微博提供了相应的api来获取公开微博，但是它的api原理是一个HTTP轮询(POLLING)协议，不是即时推送(realtime push)协议。因此即使增大刷新频率也无法完全达到即时获得最新信息效果。所以微博官方推荐的更新频率为2-3分钟一次，要在短时间内获取大量的微博数据显然不可以用调用api的方式。


-------------------------------2017.6.9--------------------------------------------------
打算使用python语言编写此项目。因为github上的微博爬虫项目多为python语言编写，并且我找到了一个质量不错的项目follow  https://github.com/ResolveWang/WeiboSpider
该项目实现了包括用户信息、用户主页所有微博、微博搜索、微博评论和微博转发关系抓取等。最重要的是使用了python3并且现在在持续更新中。
项目为分布式爬虫，也支持单机单节点爬取。原作者使用了云主机，我也打算用学生账户来申请两个云主机来进行爬取。

第一天上手读源码，感觉python与java源码阅读上的感觉区别在于读java代码时对于用户定义类便于查找阅读，在python中一个函数返回的变量不知道这个变量的意义
看了python的基础语法，还有很多包如urlib还不会用

Step1模拟登陆
写模拟登陆的前提是要分析并理解新浪微博的登录过程，两篇博客介绍的很详细  http://blog.csdn.net/pipisorry/article/details/47904355  http://blog.csdn.net/nobody_wang/article/details/60957893  微博登录的用户名采用base64加密，密码经过了三次rsa加密， 且其中加入了 servertime 和 nonce 的值来干扰


-------------------------------2017.6.10--------------------------------------------------
因为采用了python3 所以 python2中许多库都安装不了，譬如urillib2，所以爬虫的时候直接采用了requests库来获取session
现在许多网站不使用http协议，这点通过winshark抓包证实了，使用的是https(tcp和tslv1.2协议)



-------------------------------2017.6.11--------------------------------------------------
前几天都在学习基础知识，感觉项目的进度有点慢，这样的话六月底看不完啊
在wiki上发现了作者的开发记录讲解，感觉容易了不少
http://www.jianshu.com/p/efcf030e68c5	
http://www.jianshu.com/p/816594c83c74
登录部分看懂了一小部分，感觉对于python的基础语法不了解导致切出去查api耗费太多时间
还有python的api跟java比好难看。。。



-------------------------------2017.6.11--------------------------------------------------
关于sso转跳的微博分析
http://blog.csdn.net/pipisorry/article/details/47904355

有一个问题就是在重定向的时候转跳的url后边接的ticket的参数不知道从哪里找，最后通过抓包发现在302前会有一个post操作，在报文中会有参数，所以编程的时候传入就好




-------------------------------2017.6.13--------------------------------------------------
在用户密码加密的JavaScript部分，有两种加密算法，一个是rsa一个是三次hex加密。在rsa加密中，调用了微博的RSAKey.encrypt方法会返回一个十六进制的数据，但是在python中加密完只是返回字节数据Raw Bytes。所以需要转换成十六进制才可以
SHA1加密blog   http://blog.csdn.net/monsion/article/details/7981366
因为采用一种加密方式即可，所以采用的是rsa加密



-------------------------------2017.6.20--------------------------------------------------
开始页面解析，微博分为两种用户，一种是个人用户另一种是企业用户。两种用户的界面有不同的地方，但是对于两个页面相同的地方则进行相同处理，所以新建一个public类用来处理共有的页面
经过对于页面源码的查看
	
div class="WB_miniblog"为微博的整个界面

	div class="WB_miniblog_fb"跟上一个整个界面相比少了最顶部的导航栏即左上角带有微博图标的白色导航栏，这个导航栏在页面被拖动时依然在最上方悬浮
		div id="pl_common_top"装有顶部导航栏的（容器）？高度为0
			div class="WB_global_nav WB_global_nav_v2 UI_top_hidden "整个导航栏，其下还有好多标签，不再细写
		div class="WB_main clearfix"	除去顶部的导航栏以后的整个界面
		div class="WB_frame_a"显示关注与私信用户上部分包括主页、服务、相册
		div id="plc_main" 上部分的下面
			div class="WB_frame_b"的为用户的关注，粉丝数，认证等
			div class="WB_frame_c"的为用户发过的微博
			



-------------------------------2017.6.21--------------------------------------------------
现在对于应该看什么有点没有方向，从预登陆完成以后任务就开始分具体的方向了，但差不多也是先获取页面，然后对页面进行解析，对于不同种的任务抓取可能要去抓包来取猜测应该填入的url，譬如说评论的抓取。
decorator类没有理解透彻





-------------------------------2017.6.24--------------------------------------------------
我日，今天装驱动时候忘了保存文本了，我去之前打的全没了。。。

今天做系统分析的大作业的时候跟山哥讨论了一下解耦，之前对于耦合的了解只限于面向对象编程的课上所说的高内聚，低耦合。让两个类之间的耦合度尽量低就是好的设计。因为如果软件的需求分析不断变化，低耦合的设计便于修改，否则就是牵一发而动全身。拿生产者消费者模型进行举例，若是消费者直接调用生产者的方法的话，那么这两个类之间就是依赖关系，如果这样写的话，消费者是阻塞的，生产者若是生产慢就需要阻塞等待。而在生产者和消费者中间加入一个缓冲区，这样就不需要消费者直接调用生产者，降低了耦合度（解耦）。进一步的想，在数据流/管道的设计模式中也用到了这样的思想，就是在不同的数据处理模块之间加入管道来降低各个模块之间的耦合度。当然数据流/管道设计模式的思想不止这一个。

生产者消费者模型是用来解决并发问题的

Celery（芹菜）支持使用任务队列的方式在分布的机器、进程、线程上执行任务调度。然后我接着去理解什么是任务队列。任务队列是一种在线程或机器间分发任务的机制。
celery钟中还存在一个消息队列，消息队列的输入是工作的一个单元，称为任务，独立的职程（Worker）进程持续监视队列中是否有需要处理的新任务。

消息通信，通常使用中间人（Broker）在客户端和职程间斡旋。这个过程从客户端向队列添加消息开始，之后中间人把消息派送给职程，职程对消息进行处理。
Celery 系统可包含多个职程和中间人，以此获得高可用性和横向扩展能力。

Celery的架构由三部分组成，消息中间件（message broker），任务执行单元（worker）和任务执行结果存储（task result store）组成。
消息中间件：Celery本身不提供消息服务，但是可以方便的和第三方提供的消息中间件集成，包括，RabbitMQ,Redis,MongoDB等，这里我先去了解RabbitMQ,Redis。
任务执行单元：Worker是Celery提供的任务执行的单元，worker并发的运行在分布式的系统节点中
任务结果存储：Task result store用来存储Worker执行的任务的结果，Celery支持以不同方式存储任务的结果，包括Redis，MongoDB，Django ORM，AMQP等。本项目使用的是redis



decorator这个类貌似挺有意思的，主要功能为将一个函数进行包装，再返回一个函数，这个返回的函数是具有包装器的上下文（context）的。
blog地址：	http://www.cnblogs.com/SeasonLee/articles/1719444.html





-------------------------------2017.6.25--------------------------------------------------
看了一下funtools这个库，这个库主要有两个函数，一个是wraps，一个是partial
这个库也使用了decorator类
使用wraps是为了防止类名和类文档被修改，可以保持原函数的__module__、__name__、__doc__，还有其他参数可选。
partial是就是帮助我们创建一个偏函数的，即用一些默认参数包装一个可调用对象,返回结果是可调用对象，并且可以像原始对象一样对待，应该就是将一些默认值传入函数中，将这个函数返回。

具体的blog地址  	http://blog.csdn.net/lwnylslwnyls/article/details/48007001


python会有一个缺陷就是引入一个包的时候如果import*的话会存在调用不该调用的函数的问题，为了解决这个问题，python引入了_all_变量。该变量定义了这个包或者模块下可以被外部引用的属性或者方法（只针对在别的文件下使用import*from。。。时有效，对import具体属性/方法from。。。无效）






-------------------------------2017.6.26--------------------------------------------------
关于http的发送 	

blog:     http://blog.csdn.net/21cnbao/article/details/56275456

http请求由三部分组成：请求行、  请求头和请求正文。



请求行就是第一行，是“方法 URL  协议/版本”，并以 回车换行作为结尾。请求行以空格分隔。格式如下：
POST /index.php HTTP/1.1
以上代码中“POST”代表请求方法，“/index.php”表示URI，“HTTP/1.1代表协议和协议的版本
还有其他的方法，但是常用的有POST和GET。
GET 方法用于获取由 Request-URI 所标识的资源的信息，GET方法是默认的HTTP请求方法，例如当我们通过在浏览器的地址栏中直接输入网址的方式去访问网页的时候，浏览器采用的就是 GET 方法向服务器获取资源。我们可以使用GET方法来提交表单数据，用GET方法提交的表单数据只经过了简单的编码，同时它将作为URL的一部分向服务器发送，因此，如果使用GET方法来提交表单数据就存在着安全隐患上。例如：
         Http://localhost/login.php?username=aa&password=1234
从上面的URL请求中，很容易就可以辩认出表单提交的内容。（？之后的内容）另外由于GET方法提交的数据是作为URL请求的一部分所以提交的数据量不能太大。这是因为浏览器对url的长度有限制
       各种浏览器也会对url的长度有所限制，下面是几种常见浏览器的url长度限制:(单位:字符)
IE : 2803
Firefox:65536
Chrome:8182
Safari:80000
Opera:190000 

POST的方法是对于Get方法的替代方法。它主要是向Web服务器提交表单数据，尤其是大批量的数据。 在请求头信息结束之后的两个回车换行之后（实际是空一行），就是表单提交的数据

POST方法克服了GET方法的一些缺点。通过POST方法提交表单数据时，数据不是作为URL请求的一部分而是作为标准数据传送给Web服务器，这就克服了GET方法中的信息无法保密和数据量太小的缺点。因此，出于安全的考虑以及对用户隐私的尊重，通常表单提交时采用POST方法。
　 从编程的角度来讲，如果用户通过GET方法提交数据，则数据存放在QUERY＿STRING环境变量中，而POST方法提交的数据则可以从标准输入流中获取。

GET与POST方法有以下区别：
      1、  在客户端，Get方式在通过URL提交数据，数据在URL中可以看到；POST方式，数据放在HTTP包的body中。
      2、 GET方式提交的数据大小有限制（因为浏览器对URL的长度有限制），而POST则没有此限制。
      3、安全性问题。正如在（1）中提到，使用 Get 的时候，参数会显示在地址栏上，而 Post 不会。所以，如果这些数据是中文数据而且是非敏感数据，那么使用 get；如果用户输入的数据不是中文字符而且包含敏感数据，那么还是使用 post为好。
      4.、服务器取值方式不一样。GET方式取值，如php可以使用$_GET来取得变量的值，而POST方式通过$_POST来获取变量的值。



然后是请求头，每个头域由一个域名，冒号（:）和域值三部分组成。域名是大小写无关的，域值前可以添加任何数量的空格符，头域可以被扩展为多行，在每行开始处，使用至少一个空格或制表符。
头域包括Transport头域，Client头域，Cookie头域，Entity头域等等。




在接收和解释请求消息后，服务器会返回一个 HTTP 响应消息。与 HTTP 请求类似，HTTP 响应也是由三个部分组成，分别是：状态行、消息报头和响应正文。


状态行由协议版本、数字形式的状态代码，及相应的状态描述组成，各元素之间以空格分隔，结尾时回车换行符，

 状态代码与状态描述
      状态代码由 3 位数字组成， 表示请求是否被理解或被满足，状态描述给出了关于状态码的简短的文字描述。状态码的第一个数字定义了响应类别，后面两位数字没有具体分类。第一个数字有 5 种取值，如下所示。
1xx：指示信息――表示请求已经接受，继续处理
2xx：成功――表示请求已经被成功接收、理解、接受。
3xx：重定向――要完成请求必须进行更进一步的操作
4xx：客户端错误――请求有语法错误或请求无法实现
5xx：服务器端错误――服务器未能实现合法的请求。


常见状态代码、状态描述、说明：
200 OK      //客户端请求成功
400 Bad Request  //客户端请求有语法错误，不能被服务器所理解
401 Unauthorized //请求未经授权，这个状态代码必须和WWW-Authenticate报头域一起使用 
403 Forbidden  //服务器收到请求，但是拒绝提供服务
404 Not Found  //请求资源不存在，eg：输入了错误的URL
500 Internal Server Error //服务器发生不可预期的错误
503 Server Unavailable  //服务器当前不能处理客户端的请求，一段时间后可能恢复正常




Docker

blog:   http://blog.csdn.net/21cnbao/article/details/56275456
Docker直译为码头工人。当它成为一种技术时，做的也是码头工人的事。官网是这样描述它的：“Docker是一个开发的平台，用来为开发者和系统管理员构建、发布和运行分布式应用。”也就是说，如果把你的应用比喻为货物，那么码头工人（Docker）就会迅速的用集装箱将它们装上船。快速、简单而有效率。它是用Go语言写的，是程序运行的“容器”（Linux containers），实现了应用级别的隔离（沙箱）。多个容器运行时互补影响，安全而稳定。

其实我最开始看到Docker的定义时完全没有明白Docker是干什么的。通过一个比较，我们可以快速的了解Docker的大致功能。KVM, Virtualbox, Vmware是虚拟出机器，让每个实例看到一个单独的机器，那么可以说Docker是虚拟出操作系统，实现应用之间的隔离，让各个应用觉得自己有一个自己的操作系统，而且彼此之间隔离。假设没有Docker，然后有进程1和进程2，进程1和进程2共享kernel，它们是同一OS下2个进程，因此必须拥有不同PID，但是又共享网卡，共享IP地址，看到一样的根文件系统（不chroot的情况下）等，可以用Linux IPC手段进程间通信。有Docker的情况下，假设进程1和进程2运行于不同的容器，那么进程1和进程2都觉得自己和对方没有半毛钱关系，都觉得自己拥有自己的根文件系统，自己的网卡等，然后进程1和进程2的PID还可以一样，比如假设2个都是100。但是，此100非彼100。
Virtualbox等虚拟机的思路则完全不一样，如果进程1和进程2运行于不同的虚拟机，则操作系统都是双份的，它们感觉自己在不同的虚拟电脑上面跑。
由于可见，Docker达到了类似虚拟机的效果，但是又没有虚拟机的开销，它虚拟的层次更加高。Docker不虚拟机器，仅仅虚拟应用的运行环境。

Docker中可能涉及到3个机器或者更多机器，一个运行docker命令的client， 一个包含images并以容器(container)形式运行image的主机，一个docker的images仓库。client与docker host上面的docker daemon通信。当然docker client和host可以运行于一台机器（我们做实验的时候是一台），默认的docker仓库是Docker Hub。
一般的流程中,client发pull命令从仓库把image拉到docker host，然后通过run命令指挥image到host上面弄一个container来跑这个image。
当然也可以是相反的流程，client 通过build命令在host上面创建一个自己的image，然后通过push命令把image推到仓库。之后这个image可以被别的人或者自己pull。
所以什么是image，image为特定目的而生，比如弄了个nginx的image后，这个image就把nginx的东西包罗万象了，无论是张三、王五、六麻子还是七癞子，无论它是什么电脑，什么操作系统，只要支持docker，它把这个nginx的image下载下来后，拿docker run命令就可以弄容器跑nginx了。这样，用户就不用装nginx以及它依赖的一切包了。



Threading 
blog:	http://blog.csdn.net/eastmount/article/details/50155353
python中的Threading模块是对Thread模块的二次封装，Threading的调用有三种，具体看blog。



对于多重装饰器，首先执行最后一行@的装饰器，然后将返回的函数传回上一行的@装饰器，反复如此直到第一行。

在util_cls中的KThread这个线程类没有看的太懂，应该重写run方法，结果在重写的start方法中之间将run方法赋给其他方法，然后在将其他方法赋给run。有点懵
还有就是关于sys.settrace()函数.
今天看的有点头疼感觉这些可以明天再看。今天先到这




-------------------------------2017.7.6--------------------------------------------------
有段时间没有看，15号就要优研计划了，十天左右要把这个项目看完然后做一个介绍的ppt，有点烦。

今天考虑了一下爬虫架构的问题，不像网上大多数的python爬虫架构采用scrapy架构，这个项目的爬虫架构思路是采用单线程多节点爬虫，所以总的来说应该是request+celery+redis的架构。
在做项目展示的时候可以对比两种架构，
两个具体介绍scrapy的架构的blog
	http://www.jianshu.com/p/a8aad3bf4dc4
	http://python.jobbole.com/86584/

微博在用户的关注和粉丝列表界面重用的展示代码，所以在抓取粉丝或者抓取关注者的时候也可以使用相同的代码




-------------------------------2017.7.8--------------------------------------------------
前两天其实也在看，只不过是在纸上写出了程序不同任务的调用流程。具体可以看项目记录的doc。
今天是七月八号，感觉现在应该着手做展示ppt了，还有一个具体的图形展示的task还没做，在结果展示的时候可以使用航母访问香港，或者西电相关的新闻。




-------------------------------2017.7.9--------------------------------------------------
有关fiddler最前面图标含义的blog
http://blog.csdn.net/ohmygirl/article/details/17849983/
更详细的
http://blog.csdn.net/andrewpj/article/details/45440097
	

